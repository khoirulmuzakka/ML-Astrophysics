{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 5-1: Warmup questions\n",
    "***\n",
    "1.  Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio\n",
    "to 95%. How many dimensions will the resulting dataset have?\n",
    "2.  Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "3.  Compute the average distance of two points in a unit square. Test your result with a code\n",
    "that uses random points.\n",
    "\n",
    "## Exercise 5-2 Principal component analysis\n",
    "***\n",
    "Return to the halo catalogues from exercise sheet 1 (you can use the same notebook). Load the\n",
    "halo catalogue and prepare the data for machine learning using the same pipelines, i.e. log halo\n",
    "concentration is the label, and all other halo properties are the features.\n",
    "1.  Train a random forest regressor\n",
    "How long does it take to train it? You can use the time library of python. Evaluate the model\n",
    "on the test set (root mean squared error between labels of test set and predictions).\n",
    "2.  Use PCA to reduce the dimensionality with an explained variance ratio of 95%.\n",
    "How many dimensions does the reduced dataset have?\n",
    "3.  Train a random forest regressor on the reduced dataset How long does it take to train it?\n",
    "Evaluate the model on the test set.\n",
    "\n",
    "\n",
    "## Exercise 5-3 Manifold Learning\n",
    "***\n",
    "Same as 5.2, but now with Locally Linear Embedding (LLE), t-Distributed Stochastic Neighbor\n",
    "Embedding (t-SNE), and Isomap.\n",
    "1.  Use LLE to reduce the dimensionality. Test different numbers of dimensions for the manifold.\n",
    "How does the timing compare to PCA? Does it give better predictions?\n",
    "2.  Visualize your data with t-SNE. Use sklearn.manifold.TSNE to reduce your training set to 2\n",
    "dimensions (this may take a few minutes). Then plot the results in a scatterplot using colors to\n",
    "represent log halo concentration (i.e. the labels).\n",
    "3.  Visualize your data with Isomap. Use sklearn.manifold.Isomap to reduce your training set to\n",
    "2 dimensions (this may take a few minutes). Then plot the results in a scatterplot using colors\n",
    "to represent log halo concentration (i.e. the labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Solutions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.1.i \n",
    "$$EVR = \\frac{1}{1+\\frac{\\sum_\\tilde{i} \\lambda_\\tilde{i}}{\\sum_i \\lambda_i }}$$\n",
    "So, the number of PCA dimension for a given explained variance depends on the dataset. For a rough estimation, asssuming the dataset are completely random, we expect that all the eigenvalues have almost the same value, hence $\\frac{\\sum_\\tilde{i} \\lambda_\\tilde{i}}{\\sum_i \\lambda_i } = \\frac{D-M}{M}$, or $EVR = M/D$. Thus the resulting dimension is M = 0.95*D = 950.\n",
    "## 5.1.ii\n",
    "why not? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.1.iii\n",
    "We want to compute the average distance of two random points inside a square, then what we basically compute is the following integral :\n",
    "$$ E[dis]=\\int_0^1 \\int_0^1 \\int_0^1\\int_0^1 \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \\,\\,dx_1\\,dx_2\\,dy_1\\,dy_2$$\n",
    "The integral can be evaluated in Mathematica and the result is \n",
    "$$ E[dis] = 0.521405$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Average distance between two points using random point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5212666339924769\n"
     ]
    }
   ],
   "source": [
    "#Generate two random number\n",
    "dist= []\n",
    "for i in range(1, 100000):\n",
    "    r1 = np.array([random.random(), random.random()])\n",
    "    r2 = np.array([random.random(), random.random()])\n",
    "    dist.append(distance.euclidean(r1,r2))\n",
    "    \n",
    "average = np.mean(dist)\n",
    "print(average)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIstribution of the distance of two random number in a box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estimate $\\pi$ using Random Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13508\n"
     ]
    }
   ],
   "source": [
    "inside = 0\n",
    "outside =100000\n",
    "for i in range(1,outside):\n",
    "    r = np.array([random.uniform(-1,1), random.uniform(-1,1)])\n",
    "    if np.linalg.norm(r)<=1:\n",
    "        inside = inside+1\n",
    "pi = 4*inside/outside\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5.2 Principle Component Analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
